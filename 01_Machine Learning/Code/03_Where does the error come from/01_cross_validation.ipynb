{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use cross validation (CV) to tune hyper-parameters and choose models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc = {\"axes.titlesize\": 20, \"axes.labelsize\": 15, \"legend.fontsize\": 15, \"lines.linewidth\": 3, \"figure.figsize\": (9, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (classification) and divide into training and testing.\n",
    "X, y = datasets.load_digits(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Hide some annoying warnings.\n",
    "\n",
    "# Enumerate (model, hyper-parameter) pair into cv.\n",
    "cv_candidate_list = []\n",
    "\n",
    "for kernel in [\"sigmoid\", \"rbf\"]:\n",
    "    for C in [1, 10]:\n",
    "        cv_candidate_list.append( SVC(kernel = kernel, C = C) )\n",
    "\n",
    "for max_iter in [100, 500, 2000]:\n",
    "    cv_candidate_list.append( LogisticRegression(max_iter = max_iter) )\n",
    "\n",
    "for activation in [\"logistic\", \"tanh\", \"relu\"]:\n",
    "    for solver in [\"lbfgs\", \"sgd\", \"adam\"]:\n",
    "        cv_candidate_list.append( MLPClassifier(activation = activation, solver = solver) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 8/16 [00:16<00:24,  3.09s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score    # Only return scores.\n",
    "# from sklearn.model_selection import cross_validate    # Return time as well.\n",
    "\n",
    "scoring = \"accuracy\"\n",
    "scoring = \"f1_macro\"\n",
    "\n",
    "cv_score_list, training_score_list, testing_score_list = [], [], []\n",
    "for cv_candidate in tqdm(cv_candidate_list):\n",
    "    cv_score_list.append( np.average(cross_val_score( cv_candidate, X_train, y_train, cv = 5, scoring = scoring)) )\n",
    "    \n",
    "    model = cv_candidate.fit(X_train, y_train)\n",
    "    training_score_list.append( model.score(X_train, y_train) )\n",
    "    testing_score_list.append( model.score(X_test, y_test) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A good cv result will have the similar performance with testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the score between training and testing.\n",
    "model_number_list = list(range(1, len(cv_candidate_list) + 1))\n",
    "plt.plot(model_number_list, training_score_list, color = \"red\",  label = \"training\")\n",
    "plt.plot(model_number_list, testing_score_list, color = \"blue\", label = \"testing\")\n",
    "plt.xlabel(\"Model index\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.title(\"training_score vs testing_score\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xticks(model_number_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training score couldn't reflect testing score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the score between cv and testing.\n",
    "model_number_list = list(range(1, len(cv_candidate_list) + 1))\n",
    "plt.plot(model_number_list, cv_score_list, color = \"red\",  label = \"cv\")\n",
    "plt.plot(model_number_list, testing_score_list, color = \"blue\", label = \"testing\")\n",
    "plt.xlabel(\"Model index\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.title(\"CV_score vs testing_score\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.xticks(model_number_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CV score can reflect testing score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
